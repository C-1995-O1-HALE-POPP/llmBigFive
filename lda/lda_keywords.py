import json
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

print("ğŸ” æ­£åœ¨è¯»å–å…³é”®è¯æ•°æ®...")

# è¯»å–å…³é”®è¯åˆ—è¡¨
try:
    with open("keywords.json", "r", encoding="utf-8") as f:
        keywords = json.load(f)
    print(f"âœ… è¯»å–æˆåŠŸï¼Œå…±æœ‰ {len(keywords)} ä¸ªå…³é”®è¯ã€‚")
except Exception as e:
    print(f"âŒ è¯»å–å¤±è´¥ï¼š{e}")
    exit()

# æ¸…æ´—å…³é”®è¯
keywords = [kw.strip() for kw in keywords if kw.strip()]
print(f"âœ… æ¸…æ´—å®Œæˆï¼Œä¿ç•™äº† {len(keywords)} ä¸ªæœ‰æ•ˆå…³é”®è¯ã€‚")

# åˆå¹¶æˆæ–‡æœ¬æ•°æ®
text_data = [" ".join(keywords)]
print("âœ… å…³é”®è¯åˆå¹¶ä¸ºä¸€æ®µæ–‡æœ¬ã€‚")

# å‘é‡åŒ–å¤„ç†
print("ğŸ”„ æ­£åœ¨è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–ï¼ˆCountVectorizerï¼‰...")
try:
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(text_data)
    print(f"âœ… å‘é‡åŒ–å®Œæˆï¼Œè¯æ±‡æ€»æ•°ï¼š{len(vectorizer.get_feature_names_out())}")
except Exception as e:
    print(f"âŒ å‘é‡åŒ–å¤±è´¥ï¼š{e}")
    exit()

# LDA æ¨¡å‹
print("ğŸ”„ æ­£åœ¨è®­ç»ƒ LDA æ¨¡å‹...")
try:
    lda_model = LatentDirichletAllocation(n_components=5, random_state=42)
    lda_model.fit(X)
    print("âœ… LDA æ¨¡å‹è®­ç»ƒå®Œæˆã€‚")
except Exception as e:
    print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼š{e}")
    exit()

# è·å–ä¸»é¢˜å…³é”®è¯
print("ğŸ“Š è¯é¢˜å…³é”®è¯æå–ä¸­...")
try:
    feature_names = vectorizer.get_feature_names_out()
    for topic_idx, topic in enumerate(lda_model.components_):
        top_keywords = [feature_names[i] for i in topic.argsort()[:-11:-1]]
        print(f"\nğŸ§  Topic {topic_idx + 1}: {', '.join(top_keywords)}")
    print("\nğŸ‰ æ‰€æœ‰è¯é¢˜æå–å®Œæˆï¼")
except Exception as e:
    print(f"âŒ æå–å¤±è´¥ï¼š{e}")
